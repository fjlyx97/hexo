---
title: 高并发系统设计
date: 2020-07-18 14:09:10
tags:
- linux
- 学习
- 并发
categories : "Linux"
---

> - 高并发系统设计笔记
> - 基于极客时间专栏

<!-- more-->


# 性能的度量指标
明确度量指标十分重要，单次响应时间没有意义，应当关注一段时间的性能是什么样的。依据一些统计方法计算出特征值，这些特征值可以反应在这段时间的性能情况。常用的指标如下：

## 平均值
这段时间响应时间相加，除以总数。这种方案敏感性较差，如果出现了慢请求，无法如实反映出实际情况

## 最大值
请求时间最长的值。和平均值相比，太过敏感。

## 分位值
分位值分为很多种，比如90分位，95分位，75分位。把这段响应时间从小到大排序，假设100个请求，排在第90位的响应时间就是90分位值。这种方案可以排除偶发的慢请求对数据的影响，很好的反应出这段时间的性能情况，分位值越大，对于慢请求的影响就越敏感。
- 分位值适合在时间段内，响应时间统计值来使用，在实际应用中也应用的最多。
- 通常描述：在每秒 1 万次的请求量下，响应时间 99 分位值在 10ms 以下。

## 分界点
从用户体验来看：**200ms**感受不到延时。**1s**，感受到延时，但是可以接收。之后等待时间越长，用户体验就越差。因此健康系统的99分位值应当控制在200ms以内。而不超过1s的请求占比要在99.99%以上。

# 高并发下的性能优化
## 提高系统的处理核心数
提高系统核心数可以提高性能，但是并不意味着无限制提高核心数就可以无限制提高性能。多核心数意味着资源争抢愈发严重。在某个临界点上继续增加并发进程数，反而会造成系统性能的下降，这就是**拐点模型**。

## 减少单次任务的响应时间
区分是CPU密集型还是IO密集型

### CPU密集型
需要大量的CPU运算，选用更高效的算法，或减少运算次数。可以通过Profile工具来找到消耗cpu时间更多的方法或者模块，如linux的perf,eBPF等

### IO密集型
IO密集型大部分操作都是等待IO完成，如磁盘IO，网络IO。解决方案：采用工具排查，一些开发语言甚至有对应的内存工具。另一类手段是通过监控来发现性能问题，对每个步骤进行分时统计，从而找到任务的消耗时间。

# 可用性度量
## MTBF（Mean Time Between Failure）
平均故障间隔，代表两次故障间隔时间，也就是系统正常运转的平均时间。这个时间越长，系统越稳定

## MTTR （Mean Time To Repair）
故障平均恢复时间。越短影响越小。

# 池化技术（数据库）
数据库连接建立，假设是4ms，查询一条sql语句假设是1ms，一个完整的周期就是5ms，一秒大概能执行200次。而数据库连接的时间占了其中的4/5。

所以利用池化技术，可以预先创建好连接（**预热操作**），如果连接不够就生成新的连接，超过上限就设置等待超时时间。

- 推荐：一般线上环境最小连接数控制在10左右，最大连接数控制在20-30左右

## 可能出现的故障
- 数据库域名IP变更，但是连接池是用旧的连接
- wait_timeout参数，连接闲置过久后，数据库会主动关闭这个连接，对业务后台是无感知的。

## 解决方案
1. 发送select 1，检测是否抛出异常，异常就从连接池中移除（推荐）
2. 获取到连接后，校验连接是否可用，可用在执行（线上不推荐）

## 线程池注意事项
如果使用了线程池，一定不要使用无界队列，大量任务堆积可能会占用大量内存空间，造成服务不可用。

# 主从读写分离（数据库）
Mysql主从复制依赖binlog，即记录mysql上所有变化，并以二进制保存在磁盘日志文件上，这个过程一般是异步的。但是出于性能考虑，主库的写入流程不会等待主从同步完成后就返回。极端情况下如binlog还没刷新到磁盘，磁盘挂掉断电等，导致binlog丢失，会造成主从不一致。

- 无限制增加从库并不能完全抵抗大量的并发。随着从库增加，从库连上来的IO线程比较多，主库资源消耗比较高，同样受限于网络带宽。一般推荐一个主库挂3-5个从库

## 延时问题
有的时候从数据库查询不到信息，一段时间后又可以查询到信息。有可能是总从延时在作怪。所以把**从库落后时间**做个一个重要的指标进行监控和报警，正常的时间是在**毫秒级别**。

## 访问数据库问题
数据库采用主从方式之后，会出现多个数据库地址，并且需要区分写入操作和查询操作。因此业界引入**中间件**在解决数据库访问问题。

### 淘宝TDDL
以代码形式内嵌运行在应用程序内部，可以看成一种数据源代理，中间件将sql语句发给某个指定的数据源来处理，并将处理结果返回。
- 优点：简单易用，不用多余部署。
- 缺点：嵌入程序，更新困难，并且缺乏多语言支持。

### 单独部署代理层
阿里巴巴开源的Cobar，基于Cobar开发出来的Mycat等。这类代理层单独部署在独立的服务器上，业务代码如使用单一数据库一样使用它。

- 优点：独立部署，方便升级
- 缺点：Sql语句跨越两次网络，从应用->代理->数据源，性能有一定损耗

# 分库分表（数据库）
分库分表后，每个节点只保存部分的数据，这样可以减少单个数据库节点和单个数据表中存储的数据量，解决存储瓶颈和查询效率。

## （数据库）垂直拆分
对数据库竖着拆分，把数据库的表拆分到不同的数据库。核心思想：专库专用
```
------------------------------
|  用户       订单      物流  |
------------------------------
            ||                 
--------    ---------   ---------
| 用户 |    |  订单  |   | 物流  |
-------     ---------   ---------
```
- 这种方案偏常规，但是并不能直接解决业务量大量增多的场景。

## （数据库）水平拆分
1. 对某一个字段hash，比较适合实体表如用户表。对用户id进行hash，再对库数取余，即可分散到各个库上去。
2. 按照某一字段划分，如时间来划分。

## （表）垂直拆分和水平拆分
表的水平拆分和库的几乎一致，但是表的垂直拆分略有不同。表的垂直拆分是将字段进行分割到多表

## 对比
垂直拆分注重业务的相关性，水平拆分注重的是将单一数据表按某一种规则分到多个数据库和数据表中，关注数据的特点。

## 分库分表带来的问题
最大的问题是引入了分区键，查询的时候必须带上这个字段，才能找到数据所在的库和表，否则就只能向所有数据库和表发送查询指令，次数为：**库数x表数**。
1. 建议：性能没到瓶颈就尽量不做分库分表
2. 要做就一次做到位，如一次性做出16库64表，满足几年内的业务需求

# 发号器（数据库）
上文的分库分表，会带来一个问题就是全局唯一性的问题。

## 数据库主键要如何选择
依据数据库第二范式，数据库中的每一个表都需要有一个唯一主键，一般来说有两种方式。
1. 使用业务字段作为主键，如用户手机号，email，身份证号。但是大多数情况不适合用，如评论表很难找到一个字段标识唯一评论。用户表一个人可以拥有多个手机和email，一旦变更手机号或者email，就需要变更所有外键信息。身份证号同理，身份证号码位数变更，一样会引起巨大变动。
2. 使用生成的唯一ID作为主键。单库单表的情况，我们可以用自增字段作为ID，这样最简单，也最透明，但是分库分表后，使用自增字段就无法保证ID的全局唯一性了。

## 基于Snowflake算法搭建发号器
### UUID
UUID（统一唯一标识码），不依赖于第三方系统，性能可用性较好，但是如果用来数据库主键，会存在以下几点问题：
1. 生产的ID无序，不能单调递增，而数据库中ID可能有被用来做特殊用途，如作为分区键、排序的用途。
2. ID有序可以保障数据库写入性能，B+数底层是有序排列的，如果插入是递增ID，只需要把他加入尾部即可。
3. UUID不具备业务含义

## 雪花算法
Snowflake 的核心思想是将 64bit 的二进制数字分成若干部分，每一部分都存储有特定含义的数据，比如说时间戳、机器 ID、序列号等等，最终生成全局唯一的有序 ID。它的标准算法是这样的：
```
0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000
    |              毫秒级时间                    | |数据中心||机器|   |毫秒内计数|
```

### 问题
Snowflake的算法完全依赖于时间，如果时钟回拨的话，可能会生成重复的ID，因此解决方案有：
1. 关闭时间同步
2. 使用阿里云的时间服务器
3. 如果回拨时间很短，如5毫秒，就等待一会

如果请求量很小的话，会造成末位毫秒内计数始终为1，如果使用了分区的技术，会造成分配不均匀，因此解决方案有：
1. 时间戳记录的是秒而不是毫秒
2. 生成的序列号的起始号做一下随机，尽量均衡

## 部署
1. 嵌入业务代码，好处是不需要跨网络调用，性能会好一些，但是需要更多的机器ID来支撑业务（确保机器ID唯一，需要通过zookeeper分布式一致性组件来保证机器重启一定获得唯一的机器ID
2. 独立服务部署（发号器服务），缺点：多一次网络调用，但是内网调用性能损耗有限，并且机器ID可以写在配置文件中（量少）。微博美团都是使用独立服务部署，性能上单实例单cpu可以达到2万每秒

# 缓存
缓存不一定是代表把数据放于内存中存储。凡是位于速度相差较大的两种硬件之间，用于协调两者数据传输速度差异的结构，都可以称为缓存。
- **缓存命中率是我们缓存最重要的一个监控项。**

## 缓存的不足
1. 缓存适合读多写少的场景，并且数据最好带**热点属性**，如果缺少了热点属性，缓存的效应就不明显了。
2. 缓存提升系统的整体复杂度，并且会有数据不一致的风险。如更新数据库成功，更新缓存失败，缓存中就会存在脏数据。通常我们会设置较短的过期期限，或者手动清理。
3. 需要评估内存，内存有限。

# 选择缓存的读写策略（重要）
针对不同场景，需要考虑不同的策略。并且需要考虑诸多因素，如缓存是否可能**被写入脏数据**，策略的**读写性能**如何，是否存在缓存的**命中率下降**

## Cache Aside旁路策略（常用）
考虑如下场景：A请求将数据库id从20变21->B请求将数据库从21变22->B请求写入缓存->A请求写入缓存。这首和会造成更新丢失
- 解决方案：更新数据的时候（先），同时删除缓存（后），等待下次查询缓存时，重新从数据库中读取。**顺序不能调换**

最大的问题：当写入比较频繁时，缓存也会被清空的比较频繁，会对缓存的命中率造成一定的影响。

## （读穿/写穿）策略
这个策略的核心在于，用户只和缓存打交道，写入或者读入数据。

- 这种方案用的比旁路策略少，常用的缓存如Memcached和Redis都不提供写入数据库，或者自动加载数据库的功能。

### Write Through
1. 查询数据在缓存中是否存在，存在直接写入缓存，并由缓存组件同步更新到数据库中。
2. 如果不存在的话，被称为（Write Miss写失效)，这时候衍生出两种解决方案：1）写入缓存相应位置，再有缓存组件同步到数据库。2）直接更新到DB（推荐）

## Write Back写回策略
核心思想：写入数据时只写入缓存，并把这块缓存标记为脏。而脏块只有再次被使用的时候，才会写入db（不能被使用在正常场景，计算机体系中磁盘的策略）

# 分布式缓存的高可用方案
## 客户端方案
在客户端配置多个缓存节点，可以自定义负载均衡算法，来实现分布式
- 再写入数据时，把数据分散到多个节点中
- 读数据时，可以使用主从或者多副本策略

### 数据分片
数据可以使用Hash分片算法和一致性Hash分片。一致性Hash算法有几率产生脏数据，客户端写入k=3到A中，这时候更新K=4，刚好A的连接出现问题。K=4会被更新到B节点当中，A恢复后就会产生脏数据。

- 因此一致性Hash算法一定要设置缓存的过期时间。
- 推荐4-6个节点来分片

## 中间层代理
在应用代码和缓存节点中间增加代理层，客户端所有的写入或者读取请求都走代理层，由代理层实现高可用策略。可以降低耦合性，如所有语言公用一套环境。
- Mcrouter、Twemproxy、Codis

## 服务端方案
由redis官方提出的redis sentinel方案，在服务端进行监控实例

# 避免缓存穿透
## 回种空值
如果数据库查不到值，我们就向缓存回种一个空值（比如查不到uid，就将uid作为key，value为空值写入缓存）。这个空值不存放业务数据，但是占用缓存空间，所以需要加一个较短的过期时间。

- 如果有大量空值的话，需要评估容量，防止出现空值过多淘汰正常数据，造成缓存命中率下降

## 布隆过滤器
原理略过，布隆过滤器有两个缺陷：
1. 判断元素是否在集合有一定的错误率，有可能判断元素在集合中，但是实际不在。但是如果元素不在集合中，就一定不在集合中
2. 不支持删除元素，删除对应的bit可能会影响到多个key。解决方案：bit换为普通的int进行计数，但是会增大空间消耗。

# CDN加速
在电商系统中，图片，文字静态资源，被放在了nginx等web服务器上，读请求量极大，占用了很多带宽，这时就会出现访问慢

如果考虑分部署部署，在各个地域自建机房，是一个很大的挑战。因此我们可以在业务服务器上层，加上一层CDN缓存，用来承担静态资源的读访问。

## DNS问题
CDN加速会提供一个域名，我们需要解析自己的域名CNAME到域名上。这时候会涉及一个DNS解析的快慢问题。DNS迭代有可能达到秒级别，因此有一种解决思路是在App启动时，预先把解析的结果缓存到本地的LRU缓存里面。当我们要使用这个域名的时候，只需要从缓存中拿到IP即可。如果不存在才重启dns解析。

# 数据库迁移
如果时单库迁移单库，直接用mysql主从同步或者mysqldump工具导出即可。

1. 迁移应当是在线迁移，在迁移的同时也会有数据写入
2. 迁移要保证完整性，新库和旧库的数据是一致的
3. 迁移过程要可以回滚

## 双写方案
1. 将新库配置为源库的从库，用来同步数据。如果是多库多表，可以用第三方工具获取binlog的增量工具，按分库分表逻辑写入新库中
2. 业务代码同时写入新库和旧库。写入新库失败的需要记录下来。

# 消息队列保证消息只被消费一次
## 消费生产过程中丢失消息
消息队列一般设置在内网，如果网络波动可能造成生产失败，因此我们需要设置重传机制（2-3次）。

目前消息丢失主要存在三个场景：
1. 消息从生产者发送到消息队列
2. 消息在消息队列的存储场景
3. 消息被消费者消费的过程

- 对于第一种情况，推荐使用重试机制（2-3次）即可，但是这种方案有可能造成消息重复，比如网络抖动的时候，导致消息被消费两次。
- 争对第二种情况，可以将磁盘刷盘的间隔设置的很短，防止机器宕机和断电（几率不高，没必要）。如果对消息丢失容忍度很低，可以采用集群部署kafka服务，可以设置多个副本，**保证消息尽量不丢失**。另外：可以设置生产者**acks=all**，保证消息传到ISR集合中所有的机器。

建议：如果确保每一条消息都到达，不要开同步刷盘，选择集群部署。对消息丢失有容忍度，可以不开启ack=all，一个follower返回成功即成功

## 幂等接口
为了避免消息丢失，我们会造成两方面的影响：**性能损耗**、**重复消费**。由于消息队列主要是用于写请求，因此可以放宽条件，设置一个幂等的接口，保证消费了重复的消息，也只发生一次影响。

- kafka0.11版本中，支持“producer idempotency”，这个策略为每个生产者赋予唯一的ID，且为每条消息赋予唯一的ID。（争对生产端）
```go
isIDExisted := selectByID(ID)
if isIDExisted {
    return
} else {
    process(message) //消息不存在，处理消息
    saveID(ID)       //保存全局ID到数据库中
}
```

# 降低消息队列系统延时
## 监控延时的方式
1. 消息队列工具，监控消息的堆积
2. 生成监控消息的方式来监控延时（监控程序使用不同的Consume Group）

## 减少方式
两个层面：**消费端**，**消息队列**

### 消费端
1. 优化消费代码提高性能
2. 增加消费者数量（Kafka不适合用）

# 每秒一万次请求需要做服务化拆分吗
体量大意味着每一次发布部署，回归测试，都需要很长的时间，这时候就可以启动服务化拆分。

# RPC调用
## TCP_NODELAY
开启后会禁用Nagle算法。Nagle算法主要是当发送数据量级很小时，会进行缓存数据。**连续的小数据包，大小没有一个MSS，并且没有收到上一个包的ACK（或者等待DelayedACK时间，默认40ms）**，就会进行缓存。在这个算法下，会造成网络延时上升，对时延要求很小的系统不利。

## 序列化协议
- 性能要求不高，传输数据占用带宽不大的而场景下，可以使用Json作为序列化协议
- 性能要求比较高，使用Protobuf或者Thrift
- 存储场景，缓存中的存储数据占用空间较大，可以考虑protobuf

# 负载均衡
LVS工作在OSI网络模型第四层，Nginx工作在第七层。在架构中，我们一般同时部署LVS和Nginx，入口处选择LVS，将流量分发到多台Nginx上。

- LVS在四层转发，请求包转发后，客户端和后端服务直接建立连接，后续的响应包不在经过LVS，所以和nginx相比，性能更高。
- 第四层转发，无法对URL进行细分，且也没有检测后端服务是否存活的机制。
- 建议：QPS十万内，可以考虑不引入LVS，而选择nginx作为唯一的负载均衡服务器

## 微服务为什么不用Lvs和Nginx负载均衡
LVS和Nginx都是适用于普通的web服务，对于微服务架构来说，他们是不合适的。Lvs很难和注册中心进行交互，获取全部节点列表。并且微服务架构中使用的不一定是Http协议，所以Nginx也不适用。

# api网关
API网关不是一个开源组件，而是一种架构模式，可以看成是系统的边界，对**出入的流量进行掌控**。网关可以分位两类：入口网关、出口网关。

- 网关注重性能和扩展性，如果网关提升10ms，整个系统都会因此受累。

## 入口网关
入口网关一般被放置在负载均衡服务器和应用服务器之间，入口网关作用很多：隔离客户端和微服务，提供Http->Rpc协议转换，安全策略，认证，限流，熔断等功能。

## 出口网关
用来调用第三方服务，提供统一的接口，在其中可以对调用外部的API做统一的认证，授权，审计，访问控制。

## 网关种类
1. Kong：Nginx里运行的Lua程序
2. Zuul: Spring Cloud全家桶成员
3. Tyk:  Go语言实现的轻量级Api网关

# 多机房部署
在不同的IDC机房，部署多套服务，共享同一份业务数据，承担流量。

## 两种思路
1. B机房直接跨机房读取A的从库
2. B机房也部署从库，同步A机房数据

- 北京同地双机房专线延迟在1ms-3ms
- 国内异地双机房之间的专线延迟在50ms以内

# Service Mesh
Service Mesh主要处理服务之间的通信，主要表现形式：在应用程序同机器上部署一个代理程序（Sidecar边车）。服务间的通信也从客户端和服务端直连变成了：Rpc客户端->Sidecar->SideCar->Rpc服务端

# 监控系统
## 如何选择监控指标
谷歌经验总结出四个黄金信号：延迟，通信量，错误，饱和度
- 延迟：接口响应时间、访问数据库，缓存的响应时间
- 通信量：可以理解成吞吐量，单位时间内请求量的大小。
- 当前系统的错误量。有一些隐式错误，如Web服务出现了4xx或者5xx。
- 饱和度：机器资源的上限程度，CPU使用率，内存使用率，磁盘使用率，缓存数据库的连接数。

## 如何采集数据指标
### Agent
Agent是一种比较常见，采集指标的方式。通常在数据源服务器上，部署自研或者开源的Agent，来采集数据

### 日志系统
- Apache Flume
- Fluentd
- Filebeat

一般使用两个消息队列处理程序：第一个数据写入到ElasticSearch，通过Kibana展示。另外一个消息队列写入到流式处理的中间件，如Spark、Storm。

# 熔断机制
## 断路器模式
服务调用放为每一个服务维护一个有限状态机。状态机有三种状态：关闭（允许调用远程服务），半打开（尝试调用远程服务），打开（返回错误）。
- 失败次数累积到一定与之，状态从关闭变成半打开，**一般实现时，状态变更后会清零次数**
- 当处于打开状态，会启动一个超时计时器，计时器到点后，重新返回半打开状态
- 当处于半打开状态，请求可以到后端，统计成功和失败的次数，如果调用失败，进入打开状态

# 降级机制
为系统添加开关，区分核心服务和非核心服务。只能争对**非核心服务进行降级**

# 限流算法
## 固定窗口
固定每秒钟N次，但是如果在一秒的尾部，如900ms和一秒内的头部并发流量，将导致策略失效
```
|   1s   |   1s   |
       |...|
```
## 滑动窗口
参考TCP滑动窗口协议

## 漏桶算法
利用消息队列模拟漏桶，在流量产生端和接收端之间增加一个漏通，使流量变得平滑，多余的流量触发限流策略，被拒绝服务。这种方案回**导致流量响应时间变长**。

## 令牌桶算法
如果假设限流一秒内为N次，则每隔1/N的时间，往桶内放入一个令牌。流量需要先拿到令牌，才可以继续下一步。对**令牌总数**进行限制。